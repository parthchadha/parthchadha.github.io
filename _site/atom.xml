<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Parth Chadha</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2019-12-20T12:02:30-08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Parth Chadha</name>
 </author>
 
 
   
   <entry>
     <title>Test</title>
     <link href="http://localhost:4000/2019/12/06/test.html"/>
     <updated>2019-12-06T00:00:00-08:00</updated>
     <id>http://localhost:4000/2019/12/06/test</id>
     <content type="html">
</content>
   </entry>
   
 
   
   <entry>
     <title>Forward and Reverse KL divergence with JAX</title>
     <link href="http://localhost:4000/posts/2019/11/05/JAX-Forward-and-Reverse-KL-Divergence.html"/>
     <updated>2019-11-05T00:00:00-08:00</updated>
     <id>http://localhost:4000/posts/2019/11/05/JAX Forward and Reverse KL Divergence</id>
     <content type="html">&lt;h3 id=&quot;this-post-will-try-to-explain-the-differences-between-kl-and-reverse-kl-divergences-and-demonstrate-it-using-a-small-generative-model-trained-on-mixture-of-gaussian-dataset-i-will-be-using-jax-to-demonstrate-this-example&quot;&gt;This post will try to explain the differences between KL and reverse KL divergences and demonstrate it using a small generative model trained on mixture of gaussian dataset. I will be using JAX to demonstrate this example!&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from __future__ import print_function
import jax
import jax.numpy as np
from jax import jit
import random
import numpy as onp
import matplotlib.pyplot as plt
import seaborn as sns
import itertools
import matplotlib.patches as mpatches
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;create-mixture-of-gaussian-data&quot;&gt;Create mixture of gaussian data&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;p = 0.75
batch_size=64
mean_std = [[[0.5,2],[[4,0],[0,1.5]], p], [[9,9],[[1,0],[0,3]],1-p]]

def create_mixture_of_gaussian(batch_size, mean_var):
    nb_samples = batch_size
    data = [onp.random.multivariate_normal(params[0], params[1], int(nb_samples*params[2])) for params in mean_var]
    return onp.vstack(data)

def plot_dataset(ax, data, size=4, cmap=&quot;Reds&quot;, label=&quot;Ground truth&quot;):
    ax.set_aspect('equal')
    ax.set_ylim((0, size))
    ax.set_xlim((0, size))
    ax.tick_params(labelsize=10)
    sns.kdeplot(
        data[:, 0],
        data[:, 1],
        cmap=cmap,
        ax=ax,
        )
    
data_indices = [create_mixture_of_gaussian(batch_size, mean_std) for _ in range(10000)]
data = onp.vstack([d for d in data_indices])
data = (data - onp.min(data))/(onp.max(data) - onp.min(data))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;first-lets-use-mle-to-estimate-parameters-of-a-gaussian-that-fits-the-underlying-data-distribution&quot;&gt;First, lets use MLE to estimate parameters of a Gaussian that fits the underlying data distribution&lt;/h3&gt;

&lt;p&gt;Maximum Likelihood Estimation to estimate parameters of a gaussian distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_{ML} = \frac{1}{N} \sum_{n=1}^{N} x_{n}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_{ML} = \frac{1}{N} \sum_{n=1}^{N} (x_{n} - \mu)(x_{n} - \mu)^T&lt;/script&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Plot the ground truth data
fig, ax = plt.subplots(figsize=(6, 12))
plot_dataset(ax, data[:1000], size=1)

# Estimate the mean and covariance from data
mu_estimate = onp.mean(data, axis=0)
covar_estimate = onp.matmul(onp.transpose(data - mu_estimate),(data-mu_estimate)) / data.shape[0]
mean_var_estimate = [[mu_estimate, covar_estimate, 1.0]]

# Generate data from estimated mean and variance
gen_data_indices = [create_mixture_of_gaussian(batch_size, mean_var_estimate) for _ in range(100)]
generated_data = onp.vstack([d for d in gen_data_indices])
plot_dataset(ax, generated_data, size=1.2, cmap=&quot;Blues&quot;, label='Generated data')

# Add legends to the plot
groundtruth_patch = mpatches.Patch(color=sns.color_palette(&quot;Reds&quot;)[2], label=&quot;Ground truth&quot;)
gendata_patch = mpatches.Patch(color=sns.color_palette(&quot;Blues&quot;)[2], label=&quot;Generated data&quot;)
ax.legend(handles=[groundtruth_patch, gendata_patch], loc=&quot;upper left&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/JAX%20Forward%20and%20Reverse%20KL%20Divergence_6_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kullback-Leibler divergence (KL divergence) is a measure of the information lost when q(x)is used to approximate p(x), where both p(x) and q(x) are probability distributions.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{KL}(P||Q) = \mathbf{E}_{x\backsim\ p} \bigg[log \frac{P(X)}{Q(X)}\bigg]&lt;/script&gt;

&lt;p&gt;The above equation can be further transformed to :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{KL}(P||Q) = \mathbf{E}_{x\backsim\ p} \big[log P(X)\big] - \mathbf{E}_{x\backsim\ p} \big[logQ(X)\big]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{KL}(P||Q) = \mathbf{E}_{x\backsim\ p} \big[-log Q(X)\big] - H(P(X))&lt;/script&gt;

&lt;p&gt;The expected value term $\mathbf{E}_{x\backsim\ p} \big[-log Q(X)\big]$ refers to the cross entropy between P and Q and the second term $H(P(X))$ represents the entropy of probability distribution P. Having high entropy value encourages the model to spread out the probability mass instead of collapsing to a single point.&lt;/p&gt;

&lt;p&gt;P(X) is the true data distribution, which in our case is represented by mixture of gaussians and we will try to estimate P(X) using a model that is parametrized by $\theta$.&lt;/p&gt;

&lt;h4 id=&quot;forward-kl&quot;&gt;Forward KL:&lt;/h4&gt;
&lt;p&gt;Our goal here is to minimize the KL divergence between P(X) and $Q_{\theta}(X)$ such that there is minimum information loss and Q(X) best approximates the underlying data distribution.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{*} = \operatorname*{argmin}_\theta D_{KL}(P(x)||Q_{\theta}(x))&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{*} =  \operatorname*{argmin}_\theta  \mathbf{E}_{x\backsim\ p} \big[-log Q(X)\big] - H(P(X)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{*} =  \operatorname*{argmin}_\theta  \mathbf{E}_{x\backsim\ p} \big[-log Q(X)\big]&lt;/script&gt;

&lt;p&gt;From the above simplification we know that derivative of $H(P(x))$ with respect to $\theta$ is 0 and hence the only term to minimize is $\mathbf{E}_{x\backsim\ p} \big[-log Q(X)\big]$ which is equivalent to maximum likelihood estimation objective. This objective samples points from $P(x)$ and maximizes the likelihood of these points under Q(x). Forward KL divergence forces the $Q(x)$ distribution to spread out to cover as much points as possible under $P(X)$.&lt;/p&gt;

&lt;h4 id=&quot;reverse-kl&quot;&gt;Reverse KL:&lt;/h4&gt;
&lt;p&gt;We can state the reverse KL objective as :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{*} = \operatorname*{argmin}_\theta D_{KL}(Q_{\theta}(x)||P(x))&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{*} =  \operatorname*{argmin}_\theta  \mathbf{E}_{x\backsim\ Q_{\theta}} \big[-log P(X)\big] - H(Q_{\theta}(X))&lt;/script&gt;

&lt;p&gt;The above objective samples points from $Q_{\theta}(x)$ and tries to maximize the likelihood of these points under $P(x)$. Here the entropy term prevents $Q_{\theta}(x)$ to collapse to a single point. Reverse KL does not require $Q(x)$ to cover all points under $P(x)$ and just requires points under $Q(x)$ to have high likelihood under $P(x)$.&lt;/p&gt;

&lt;p&gt;Using reverse-KL is difficult in practice because in real world problems we donâ€™t have the true underlying distribution $P(x)$ and hence can not evaluate the term $\mathbf{E}&lt;em&gt;{x\backsim\ Q&lt;/em&gt;{\theta}} \big[-log P(X)\big]$. As mentioned in [1][2], we can use GAN which can allow minimization of divergences which are otherwise impossible to minimize directly.&lt;/p&gt;

&lt;h3 id=&quot;lets-create-a-simple-model&quot;&gt;Lets create a simple model&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from jax.experimental import stax
from jax import random
from jax.experimental.stax import Conv, Dense, MaxPool, Relu, Sigmoid, Tanh, Flatten, LogSoftmax, Softplus, LeakyRelu # neural network layers
from jax.experimental import optimizers
from jax import jit, grad

eps = 1e-10

reverse_kl=True

def loss_function(output, y): # Reverse KL
    return -np.mean(-y*np.exp(output) + (1 - y)*(output + eps + 1.), axis=0)[0] # Reverse KL divergence
    #return -np.mean(y*output - (1. - y)*(np.exp(output - 1.)), axis=0)[0] # KL divergence
        
def gen_loss(generator_params, discriminator_params, noise, y, reverse_kl=False):
    gen_out = generator_net_apply(generator_params, noise)
    prediction = discriminator_net_apply(discriminator_params, gen_out)
    return loss_function(prediction, y)
    
def disc_loss(p, x, y):
    prediction = discriminator_net_apply(p, x)
    loss_value = loss_function(prediction, y)
    return loss_value

@jit
def generator_step(i, disc_opt_state, gen_opt_state, noise, y):
    generator_params = gen_get_params(gen_opt_state)
    discriminator_params = disc_get_params(disc_opt_state)
    g = grad(gen_loss)(generator_params, discriminator_params, noise, y)
    return gen_opt_update(i, g, gen_opt_state)

@jit
def discriminator_step(i, opt_state, x, y):
    p = disc_get_params(opt_state)
    g = grad(disc_loss)(p, x, y)
    return disc_opt_update(i, g, opt_state)

def sample_generator(gen_opt_state, input_data):
    gen_param = gen_get_params(gen_opt_state)
    return generator_net_apply(gen_param, input_data)

def train_discriminator(i, disc_opt_state, real_data, fake_data):
    disc_opt_state = discriminator_step(i, disc_opt_state, real_data, np.ones((real_data.shape[0],1), dtype=np.float32))    
    disc_opt_state = discriminator_step(i, disc_opt_state, fake_data, np.zeros((real_data.shape[0],1), dtype=np.float32))   
    return disc_opt_state

def train_generator(i, disc_opt_state, gen_opt_state, noise):    
    gen_opt_state = generator_step(i, disc_opt_state, gen_opt_state, noise, np.ones((noise.shape[0],1), dtype=np.float32))
    return gen_opt_state
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;generator_net_init, generator_net_apply = stax.serial(
        Dense(2), Sigmoid
)

discriminator_net_init, discriminator_net_apply = stax.serial(
        Dense(32), LeakyRelu,
        Dense(32), LeakyRelu,
        Dense(1), Sigmoid
)
latent_size = 10
key = random.PRNGKey(onp.random.randint(0,999999))
key, subkey = random.split(key)
generator_in_shape = (-1, latent_size)
generator_out_shape, generator_net_params = generator_net_init(subkey, generator_in_shape)

key, subkey = random.split(key)
discriminator_in_shape = (-1, 2)
discriminator_out_shape, discriminator_net_params = discriminator_net_init(subkey, discriminator_in_shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Initialize adam optimizer for generator and discriminator
gen_opt_init, gen_opt_update, gen_get_params = optimizers.adam(step_size=1e-4)
gen_opt_state = gen_opt_init(generator_net_params)

disc_opt_init, disc_opt_update, disc_get_params = optimizers.adam(step_size=5e-4)
disc_opt_state = disc_opt_init(discriminator_net_params)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;batch_size = 64
for i in range(7000):
    real_data = data[onp.random.choice(data.shape[0], size=batch_size)]    
    noise = onp.random.uniform(size=(batch_size, latent_size))
    fake_data = sample_generator(gen_opt_state, noise)

    # Train discriminator
    disc_opt_state = train_discriminator(i, disc_opt_state, real_data, fake_data)    
    # Train generator
    gen_opt_state = train_generator(i, disc_opt_state, gen_opt_state, noise)    
    if i % 2000 == 0:
        print(i)
    
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Plot the ground truth data
fig, ax = plt.subplots(figsize=(6, 12))
plot_dataset(ax, data[:1000], size=1)

noise = onp.random.uniform(size=(1000, latent_size))
# Generate data from GAN
#print(noise)
generated_data = sample_generator(gen_opt_state, noise)
plot_dataset(ax, generated_data, size=1, cmap=&quot;Blues&quot;, label='Generated data')

# Add legends to the plot
groundtruth_patch = mpatches.Patch(color=sns.color_palette(&quot;Reds&quot;)[2], label=&quot;Ground truth&quot;)
gendata_patch = mpatches.Patch(color=sns.color_palette(&quot;Blues&quot;)[2], label=&quot;Generated data&quot;)
ax.legend(handles=[groundtruth_patch, gendata_patch], loc=&quot;upper left&quot;)
print(&quot;Plotting with KL divergence loss&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Plotting with KL divergence loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/JAX%20Forward%20and%20Reverse%20KL%20Divergence_13_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Plot the ground truth data
fig, ax = plt.subplots(figsize=(6, 12))
plot_dataset(ax, data[:1000], size=1)

noise = onp.random.uniform(size=(1000, latent_size))
# Generate data from GAN
#print(noise)
generated_data = sample_generator(gen_opt_state, noise)
plot_dataset(ax, generated_data, size=1, cmap=&quot;Blues&quot;, label='Generated data')

# Add legends to the plot
groundtruth_patch = mpatches.Patch(color=sns.color_palette(&quot;Reds&quot;)[2], label=&quot;Ground truth&quot;)
gendata_patch = mpatches.Patch(color=sns.color_palette(&quot;Blues&quot;)[2], label=&quot;Generated data&quot;)
ax.legend(handles=[groundtruth_patch, gendata_patch], loc=&quot;upper left&quot;)
print(&quot;Plotting with reverse KL divergence loss&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Plotting with reverse KL divergence loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/JAX%20Forward%20and%20Reverse%20KL%20Divergence_14_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://colinraffel.com/blog/gans-and-divergence-minimization.html&lt;/li&gt;
  &lt;li&gt;https://github.com/google/jax&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1606.00709&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;
&lt;/code&gt;&lt;/pre&gt;
</content>
   </entry>
   
 
   
   <entry>
     <title>Optimizing LSTM's on GPU with scheduling</title>
     <link href="http://localhost:4000/posts/2017/05/12/final_report.html"/>
     <updated>2017-05-12T00:00:00-07:00</updated>
     <id>http://localhost:4000/posts/2017/05/12/final_report</id>
     <content type="html">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In the Optim-LSTM project, we aim to produce a high-performance implementation of Long-Short Term Memory Network using Domain-Specific Languages such as Halide and/or using custom DSL. This would provide portability across different platforms and architectures.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;LSTMâ€™s are a variant of RNN and were designed to tackle the vanishing gradients problem with RNN. 
RNNâ€™s when back-propogated over a large number of time-steps, face a problem of diminished value of gradients. With the presence of memory cell in LSTMâ€™s, we can have continuous gradient flow which helps in learning long-term dependencies.
Equations describing LSTM network:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;i\_{t} = sigm(\theta\_{xi} X\_{t} + \theta\_{hi} h\_{t-1} + b\_{i})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\_{t} = sigm(\theta\_{xf} X\_{t} + \theta\_{hf} h\_{t-1} + b\_{f})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o\_{t} = sigm(\theta\_{xo} X\_{t} + \theta\_{ho} h\_{t-1} + b\_{o})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_{t} = sigm(\theta\_{xg} X\_{t} + \theta\_{hg} h\_{t-1} + b\_{g})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c_{t} = f\_{t} \cdot c\_{t-1} + i\_{t} \cdot g\_{t}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{t} = o\_{t} \cdot tanh(c\_{t-1})&lt;/script&gt;

&lt;p&gt;The cost involved in evaluating these networks is dominated by Matrix Matrix Multiplication operation, GEMM operation in BLAS libraries. A naive implementation would perform eight matrix-matrix multiplications, 4 with inputs and 4 with previous state vectors. We plan to explore various optimizations around the naive implementation such as Combining Operations, Fusing Point Wise operations etc.&lt;/p&gt;

&lt;p&gt;We plan to initially get familiar with Halide and understand more in depth about Domain Specific Languages and understand the limitations of Halide in tasks not meant for Image Processing such as LSTMâ€™s. Based on our learning and as mentioned in suggested projects, we plan to implement a custom DSL to incorporate variants of RNNâ€™s in the framework. Using the DSL, we aim to use basic blocks from cuDNN or cuBLAS for effecient operations on matrices.&lt;/p&gt;

&lt;h2 id=&quot;mid-term-checkpoint&quot;&gt;Mid-term checkpoint&lt;/h2&gt;
&lt;p&gt;We explored the implementation of LSTM using Halide and found that the scheduling policies used in Halide is not optimal/suitable for RNN type neural networks. Instead of starting with DSL implementation, we started working on CUDA implementation of LSTM networks as we think this optimization is key for the goal of term project. Once we have an optimized LSTM kernels, we can link them up with a general purpose framework. We have a generic framework setup in C++ which we can use to call our optimized kernels.&lt;/p&gt;

&lt;h2 id=&quot;post-mid-term-implementation&quot;&gt;Post Mid-term implementation:&lt;/h2&gt;
&lt;p&gt;We used the equations as described in Background section to implement the LSTM structure on CUDA. The current baseline version is a naive implementation using cuBLAS library for General purpose matrix multiplication &lt;code&gt;(GEMM)&lt;/code&gt;. Although GEMM is highly optimized for GPUs, it does not utilize all the parallelism that can be extracted as the matrix sizes are inherently small.
The current implmentation uses 8 GEMM operations, 4 for multiplying input $x_{t}$ with $\theta_{xi},\theta_{xf},\theta_{xo},\theta_{xg}$ and 4 for multiplying $h_{t-1}$ with $\theta_{hi},\theta_{hf},\theta_{ho},\theta_{hg}$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/04/image06.png&quot; alt=&quot;LSTM layer-wise structure&quot; /&gt;
&lt;em&gt;Figure 1: LSTM layer-wise structure&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In our current scheduling policy, we first cover the layer &lt;em&gt;l&lt;/em&gt;, across the sequence length and then move on to the layer &lt;em&gt;l+1&lt;/em&gt;. This implementation does not utilize the inherent parallelism provided by the LSTM structure, but it serves as a valid working baseline.&lt;/p&gt;

&lt;p&gt;Below are the key optimizations we are trying to implement.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Currently our baseline code uses cuBLAS library which does not utilize the complete resource of the GPU and also does not exploit inherent parallelism in LSTM networks. To avoid this problem, we plan to combine multiple matrix multiplications &lt;code&gt;(Instead of 4 different matrix multiplications)&lt;/code&gt;. We aim to combine 4 matrix multiplication into one GEMM kernel invocation. This should help in exploiting the available GPU resources. This is due to the usage of all the SMâ€™s on GPU.&lt;/li&gt;
  &lt;li&gt;As shown in Figure 1, once the computation of first time sequence of first layer &lt;code&gt;(L0,0)&lt;/code&gt; is completed, in our naive implementation we move on to second time step &lt;code&gt;(L0,1)&lt;/code&gt; in the same layer. However, since we have the computed output from &lt;code&gt;(L0,0)&lt;/code&gt;, we can also parallely work on &lt;code&gt;(L1,0)&lt;/code&gt; cell. This inherent parallelism increases as we progress through the network and more LSTM cells can be computed in parallel. This scheduling policy is expected to give the most boost in GFLOPS.&lt;/li&gt;
  &lt;li&gt;Utilizing the FMA units available on the GPU to reduce the number of times a matrix is accessed from memory. For example, we need to perform two matrix multiplcation and one addition to compute $i_{t}, f_{t}, o_{t}, g_{t}$. Instead of storing the intermediate results in a buffer, we plan to compute this in the form of $Y = AX + B$ such that the FMA units are utilized.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;current-runtime&quot;&gt;Current Runtime&lt;/h2&gt;
&lt;p&gt;The runtime for our naive implementation of a 4 layer LSTM with below configurations is 182.5ms.&lt;/p&gt;

&lt;p&gt;LSTM Config:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Number of Layers - 4&lt;/li&gt;
  &lt;li&gt;Sequence Length - 100&lt;/li&gt;
  &lt;li&gt;Hidden Unit Size - 512&lt;/li&gt;
  &lt;li&gt;Batch Size - 64&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With our proposed optimizations, we expect the runtime to reduce significantly.&lt;/p&gt;

&lt;h2 id=&quot;optimizations&quot;&gt;Optimizations&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/Figure2.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;*Figure 2 : Optimizations&lt;/p&gt;

&lt;p&gt;As show in Figure 2, we implemented multiple optimizations to optimize the LSTM forward propagation code. We achieve a speedup of 7.98x over the baseline implementation. We have used cuBLAS version 2 in our implimentation to acheive the performance gain.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Combining GEMM Operations: In our baseline implementation, we launch matrix multiplications required for LSTM computation individually. We observed that only four of the available SMs were being utilized. Our first goal was to improve the GPU occupancy and increase the resource utilized. In this optimization, we combine multiple smaller matricies and launch a combied matrix multiplication kernel. This utilizes the GPU resources in a much better manner and achieves a speedup of 2x.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Streaming GEMMs: There are multiple computations in LSTM which are completely independent of each other. Our baseline code does not exploit this feature and all the computations are done sequentially even if they are independent. Using the Nvidia Stream feature, we can launch multiple kernels which are independent of each other and perform the computation in parallel. With this optimization, we achieve a speedup of 1.82x.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fuse Point-Wise Operations: LSTM computation invloves a lot of point wise operations like tanh, addition and sigmoid. The baseline implementation launches multiple kernels to perform these operations. Launching of multiple kernels in this case is an overhead. We fuse these operations into a single kernel to overcome this inefficiency. With just this optimization, we achieve a speedup of 1.2x. The speedup with this optimization is not significant as compared to the previous speedups. The reason for this is that the amount of point-wise operations is smaller when compared to the overall computations of LSTM.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Transposing weight matrix: An obvious optimization to improve the cache locality is to transpose one of the matrix before performing the matrix multiplication. With this optimization, we achieve a speedup of 1.063x. Again, the speedup achieved with only this optimization is not significant. The matrix size is small and multiple operations are not combined into a single GEMM execution. The overhead of performing matrix transpose is high and hence the speedup is less.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Many Layer Optimization: As shown in Figure 1, once the computation of first time sequence of first layer &lt;code&gt;(L0,0)&lt;/code&gt; is completed, in our naive implementation we move on to second time step &lt;code&gt;(L0,1)&lt;/code&gt; in the same layer. However, since we have the computed output from &lt;code&gt;(L0,0)&lt;/code&gt;, we can also parallely work on &lt;code&gt;(L1,0)&lt;/code&gt; cell. This inherent parallelism increases as we progress through the network and more LSTM cells can be computed in parallel. This scheduling policy is expected to give the most boost in GFLOPS. This optimization helped us achieve a speedup of 1.30x. However, this scheduling algorithm can be used in algorithms for which we have the complete input sequence like in case of sequence to sequence models. However in case of image captioning systems, where the next step input is calculate from the first inputs LSTM output, this scheduling policy is not applicable.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The speedups mentioned in the previous section is for each individual optimization compared to the baseline. We combined multiple combinations of these optimizations and we achieve a speedup of 7.98x overall. The experiments were run with 4 layers of LSTM, each with 100 time sequences, 512 hidden units and 64 batch size.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Figure3.png?raw=true&quot; alt=&quot;&quot; /&gt;
*Figure 3: Speedup with various optimizations&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Figure4.png?raw=true&quot; alt=&quot;&quot; /&gt;
*Figure 4: Runtimes with various optimizations.&lt;/p&gt;

&lt;p&gt;Figure 3 and 4 shows the performance of various optimizations with respect to speedup and runtimes. Combining optimizations 1 and 2, we achieve a speedup of 3.74x. In this case, GEMM operations are combined to form a larger matrix and we also use Stream feature to perform independent computations in parallel. 
By adding the Fuse Point-Wise operations optimization we were able to achieve a further increase in speedup to 6.21x. As explained in the previous section,  optimization 4 did not result in any speedup. However, with all the optimizations combined, the speedup increases to 6.55x. Matrix transpose helps in achieving good cache locality, hence the improvement in performance. With our final scheduling policy, we achieve a total speedup up of 7.98x.&lt;/p&gt;

&lt;h1 id=&quot;gpu-peak-performance-analysis&quot;&gt;GPU Peak Performance Analysis&lt;/h1&gt;

&lt;p&gt;We used nvprof to profile our baseline code to evaluate the performance across various batchsizes. We calcuated the total TFLOPS the program was able to achieve by counting the number of Single Precision Floation Point operations that were performed during of program execution. We observed that with a batchsize of 224, we were able to achieve 2.16TFLOPs. This was the peak TFLOPS with our baseline code across different batch sizes. The peak TFLOPS for Nvidia GTX1080 GPU is 8.19TFLOPS and our benchmark code is significantly less.&lt;/p&gt;

&lt;p&gt;We performed a similar analysis with our fully optimized code. With an initial batchsize of 32, we achieve 2.44TFLOPS which is higher than the best case performance of the baseline. However, as the batchsize increases, the GPU hits a peak of 6.35TFLOPS and continues to remain around the same range for higher batchsizes. The comparison of the peak performance is shown in Figure 5.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Figure5.png?raw=true&quot; alt=&quot;&quot; /&gt;
*Figure 5: Peak performance of GPU vs BatchSize.&lt;/p&gt;

&lt;h1 id=&quot;matrix-factorization&quot;&gt;Matrix Factorization&lt;/h1&gt;
&lt;p&gt;After our optimizations, we explored other algorithmic changes/approximations within LSTM cell that were possible to improve the performance further. One technique was to reduce the number of weights (parameters) within an LSTM cell. Figure 6 shows the computation within an LSTM cell.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Figure6.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;*Figure 6: Matrix Factorization of LSTM&lt;/p&gt;

&lt;p&gt;In this equations T is an affine transform $T = W * [x_t,h_{t-1}] + b$. The major part of the LSTM computation is spent in computing affine transform T as it involves the matrix multiplication with W which is of size 4n x 2p, where x and h are of dimension $n$ and i,f,o,g gates are of dimension $p$.&lt;/p&gt;

&lt;p&gt;We can approximate W matrix as $W \approx W2 * W1$, where W1 is of size 2p x r and W2 is of size r x 4n. Note here we assume W can be approximated by a matrix of rank r.&lt;/p&gt;

&lt;p&gt;Total number of parameters in W earlier: 2p * 4n&lt;/p&gt;

&lt;p&gt;Total number of parameters in W after factorization: 2p * r + 4n * r&lt;/p&gt;

&lt;p&gt;Computation time comparison for a LSTM configuration of hidden unit 256 and sequence length 100. Experiments were performed on Macbook Pro.&lt;/p&gt;

&lt;p&gt;Standard LSTM   : 42.8ms&lt;/p&gt;

&lt;p&gt;Factorized LSTM : 8.36ms&lt;/p&gt;

&lt;h1 id=&quot;compiler-optimization-for-lstm-using-xla&quot;&gt;Compiler Optimization for LSTM using XLA&lt;/h1&gt;

&lt;p&gt;Google recently launched a Just-in-Time compilation toolchain for TensorFlow called XLA. This is a Accelerated Linear Algebra tool chain which fuses multiple operations within the dataflow graph of TensorFlow and generates a in-memory binary using LLVM backend. Across iterations, the same binary is invoked to perform computations. We wanted to analyze the speedup and efficiency of XLA for LSTMs and we performed a few experiments on the same. On a Intel i5 1.6Ghz CPU, we saw significant improvement in performance with XLA. We experimented with an LSTM cell of size 1024 and compared the perfomance with XLA and without XLA. The Speedup achieved with XLA as the matrix size increases from 10 to 1024. This is mainly due to the the JIT compilation overhead for smaller matricies. As shown in Figure 6, XLAâ€™s JIT compilation provides significant improvement for larger matricies.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Figure7.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;figure-7-speedup-with-xla-for-lstm-vs-matrix-size&quot;&gt;Figure 7: Speedup with XLA for LSTM vs Matrix Size.&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/Figure8.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;figure-8-memory-consumption-for-lstm-cells-with-and-without-xla&quot;&gt;Figure 8: Memory consumption for LSTM cells with and without XLA.&lt;/h3&gt;

&lt;p&gt;One of the key optimizations that XLA performs is the elimination of intermediate buffers by fusing operations. As shown in Figure 7, the memory consumption without XLA is approximately 22.5GB and 5.12GB with XLA. Due to large memory requirements, the memory bandwidth requirement increases. Due to swapping and memory latency, the cpu spends most of the time waiting for the data. XLA clearly improves the performance in this aspect for LSTMs.&lt;/p&gt;

&lt;h2 id=&quot;the-challenge&quot;&gt;The Challenge&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Since this is our introduction to Domain Specifc Languages, we assume the learning curve would be steep and hence we plan to initially spend some time to understand the code structure in Halide.&lt;/li&gt;
  &lt;li&gt;Training of LSTMâ€™s using Backpropogation Through Time is a tricky process and might take time to get the gradient calculations right. We can initially try to optimize just for inference using pre-trained network and if the time permits, optimize for backpropogation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In conclusion, we aimed to improve the performance of LSTM forward propogation on NVIDIAâ€™s GTX1080 GPU. We optimized the baseline code mainly to utilize all the available GPU resources, exploit cache locality and reduce the number of kernel invocations. We also implemented a scheduling algorithm for LSTM cells to compute independent cells in parallel. Overall, we achieved a speedup of 7.98x.&lt;/p&gt;

&lt;h2 id=&quot;platform-choice&quot;&gt;Platform Choice&lt;/h2&gt;
&lt;p&gt;The GTX 1080 GPUâ€™s on GHC.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Optimizing Performance of Recurrent Neural Networks on GPUs
 Link : https://arxiv.org/abs/1604.01946&lt;/li&gt;
  &lt;li&gt;https://devblogs.nvidia.com/parallelforall/  optimizing-recurrent-neural-networks-cudnn-5/&lt;/li&gt;
  &lt;li&gt;http://svail.github.io/diff_graphs/&lt;/li&gt;
  &lt;li&gt;http://svail.github.io/rnn_perf/&lt;/li&gt;
&lt;/ol&gt;

</content>
   </entry>
   
 
   
   <entry>
     <title>Multimodal learning techniques - 1</title>
     <link href="http://localhost:4000/posts/2016/02/07/visual_attention_2.html"/>
     <updated>2016-02-07T00:00:00-08:00</updated>
     <id>http://localhost:4000/posts/2016/02/07/visual_attention_2</id>
     <content type="html">&lt;h2 id=&quot;multimodal-distributed-representations&quot;&gt;Multimodal distributed representations&lt;/h2&gt;

&lt;p&gt;###Goal: 
We want to mathematically model the similarity between image and the sentences that describe the image. We can embed both image and sentences into an embedding space, where this embedding space has the property that vectors that are nearby are visually or semantically related. By the use of joint image-text embedding space, tasks such information retrieval becomes easy. Given a sentence, we can retrieve most relevant images from the dataset, and given an image, we can rank the best describing captions from the given set.&lt;/p&gt;

&lt;p&gt;###Model:
Paper on Visual-Semantic Embeddings [5] describes an encoder-decoder pipeline that learns a joint multimodal joint embedding space with images and text. This model encodes visual information from images and semantic information from captions into an embedding space. It uses pairwise ranking cost function to train the model.&lt;/p&gt;

&lt;p&gt;For both images and captions, we first represent them in their respective lower dimensional representations. Awesome property of these representations is that similar objects or words in this lower dimensional space are closer to each other.&lt;/p&gt;

&lt;p&gt;These lower dimensional representation are then passed through respective encoders to map them into $D$ dimensional embedding space.&lt;/p&gt;

&lt;p&gt;For images, we use CNN to represent images in a low-dimensional space using feature vectors extracted from last convolutional or fully-connected layers.
For captions, we use LSTM recurrent neural networks, which encode the sentences into $D$ dimensional space.&lt;/p&gt;

&lt;p&gt;Input to LSTMâ€™s are vector representation of sentences. Vector representation of sentences are also called word embeddings. These word embeddings can be a learnable parameter during training, else we can use pre-trained models such Word2Vec or GloVe. These pre-trained models are trained over entire Wikipedia and other huge datasets.&lt;/p&gt;

&lt;p&gt;I ran a $K-means$ clustering algorithm to cluster together words on GloVe pretrained vectors. Below are some of the results of words with similar semantic meaning.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
[u'tuesday', u'wednesday', u'monday', u'thursday', u'friday', u'sunday', u'saturday', u'late', u'night', u'morning']

[u'school', u'university', u'college', u'students', u'education', u'schools']

[u'music', u'album', u'band', u'song']

[u'game', u'play', u'games', u'played', u'players', u'match', u'player', u'playing']

[u'won', u'win', u'victory', u'gold', u'winning']

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;###Sequence of operation for captions:
Captions are parsed into words, GloVe vector is extracted for each of these word. These word vectors are then fed in sequentially into a RNN. The output from the last state of the RNN (&lt;em&gt;$D$ dimensional vector&lt;/em&gt;) represents the semantic sense of the sentence.&lt;/p&gt;

&lt;p&gt;###Cost function:
Our aim is have low cost for similar visual-semantic pairs or high cost for disimilar pairs.
For measuring similarity we use cosine similarity function (&lt;em&gt;we have same embedding dimension for image and caption&lt;/em&gt;).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Cost = \sum\_{x} \sum\_{k} max(0,\alpha - s(x,v) + s(x,v\_{k})) + \sum\_{v} \sum\_{k} max(0,\alpha - s(v,x) + s(v,x\_{k}))&lt;/script&gt;

&lt;p&gt;Here , x corresponds to image embedding, v to caption embedding, $x_{k}$ to contrastive image emedding and $v_{k}$ to contrastive caption embedding.&lt;/p&gt;

&lt;p&gt;In case of image embedding, learnable parameters are only $W_{emb}$, which map the output of CNN(&lt;em&gt;4096 dimensional vector in case of VGGnet&lt;/em&gt;) into $D$ dimensional embedding space.&lt;/p&gt;

&lt;p&gt;In case of captions, LSTM weights are the only learnable paramters.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Describing Multimedia Content using Attention-based Encoderâ€“Decoder Networks 
Link : http://arxiv.org/abs/1507.01053&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recurrent Neural Network Regularization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Show, Attend and Tell: Neural Image Caption Generation with Visual Attention&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.heuritech.com/2015/12/01/learning-to-link-images-with-their-descriptions/&quot;&gt;Heuritech blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models
 Link:&lt;/li&gt;
&lt;/ol&gt;
</content>
   </entry>
   
 
   
   <entry>
     <title>Multimodal learning techniques - 2</title>
     <link href="http://localhost:4000/posts/2016/01/21/visual_attention.html"/>
     <updated>2016-01-21T00:00:00-08:00</updated>
     <id>http://localhost:4000/posts/2016/01/21/visual_attention</id>
     <content type="html">&lt;h2 id=&quot;encoder-decoder-network&quot;&gt;Encoder-Decoder Network&lt;/h2&gt;

&lt;p&gt;It is a general framework using neural networks for mapping structured inputs to structured outputs.&lt;/p&gt;

&lt;p&gt;What do we mean by structured outputs?&lt;/p&gt;

&lt;p&gt;When we are dealing with problems such as machine translation, we need to map input(&lt;em&gt;source language&lt;/em&gt;) to an output which has its own structure(&lt;em&gt;translated language&lt;/em&gt;). As described in [1], this task is not only concerned in capturing semantics on source language, but also deals with forming coherent sentence in the translated language.&lt;/p&gt;

&lt;h3 id=&quot;encoders&quot;&gt;Encoders&lt;/h3&gt;

&lt;p&gt;The encoder reads the input data &lt;em&gt;x&lt;/em&gt; and maps into a representation &lt;em&gt;c&lt;/em&gt;. 
&lt;script type=&quot;math/tex&quot;&gt;c = f_{enc}(x)&lt;/script&gt;
In case of input as image, we use a CNN as encoder and extract features vector or &lt;em&gt;cube&lt;/em&gt; from convolutional layers.&lt;/p&gt;

&lt;h3 id=&quot;decoders&quot;&gt;Decoders&lt;/h3&gt;

&lt;p&gt;Decoder generates output &lt;em&gt;y&lt;/em&gt; conditioned on context &lt;em&gt;c&lt;/em&gt; of the input.
&lt;script type=&quot;math/tex&quot;&gt;p(y \mid x) = f_{dec}(c)&lt;/script&gt;
When are descibing image with natural language, we use RNN as a decoder.&lt;/p&gt;

&lt;p&gt;As described in [1], RNNâ€™s we use for this task are conditional language models (model distribution over sentences given an additional context &lt;em&gt;c&lt;/em&gt;).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h\_{t} = \phi\_{\theta}(h\_{t-1},x,c)&lt;/script&gt;

&lt;h3 id=&quot;issue-with-encoder-decoder-framework&quot;&gt;Issue with encoder-decoder framework&lt;/h3&gt;

&lt;p&gt;Encoder always compresses the input vector into a single fixed dimensional representation vector &lt;em&gt;c&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Not all of the images contain the same amount of information, and hence to describe the varying amount of content in the image with a single fixed dimensional vector is not a good design.&lt;/p&gt;

&lt;h2 id=&quot;attention-mechanism&quot;&gt;Attention mechanism&lt;/h2&gt;

&lt;p&gt;To tackle above discussed issues, [1] introduces a concept of attention mechanism between encoder and decoder.&lt;/p&gt;

&lt;p&gt;To incorporate structured representation of input, we want encoder to return a set of vectors describing the spatial or temporal component of input. 
We refer $c$ to as context set, which is composed of fixed size vectors. The number of vectors in each example may vary. 
&lt;script type=&quot;math/tex&quot;&gt;c = {c\_{1}, c\_{2}, \dots, c\_{M}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The attention model controls what information is seen by the decoder and hence the pipeline is composed of encoder, attention model and then decoder.&lt;/p&gt;

&lt;p&gt;The attention model takes input from hidden state of decoder at previous time step $z_{t-1}$ and score the context vector $c_{i}$. This signifies which vector $c_{i}$ is most relevant to focus on for next timestep.&lt;/p&gt;

&lt;p&gt;$e_{i}^{t}$ signifies the scores and $\alpha_{i}^{t}$ signifies the attention weights given to each element of context vector $c$.&lt;/p&gt;

&lt;p&gt;Scores $e_{i}^{t}$ are calculated as: 
&lt;script type=&quot;math/tex&quot;&gt;e\_{i}^{t} = f\_{ATT}(z\_{t-1}, c\_{i}, {\alpha\_{j}^{t-1} })&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Attention weights $\alpha$ are calculated by applying &lt;em&gt;softmax&lt;/em&gt; to scores $e$.&lt;/p&gt;

&lt;p&gt;Using attention weights and previous context vector, we can calculate the new context vector. In soft attention model, we calculate the new context vector as:
&lt;script type=&quot;math/tex&quot;&gt;c^{t} = \sum_{i=1}^{M} \alpha\_{i}c\_{i}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;With the new context vector $c^{t}$, we calculate the new state of decoder, which is RNN in image captioning case. $h_{t} = \phi_{\theta}(h_{t-1},x_{t},c_{t})$.&lt;/p&gt;

&lt;p&gt;This design of pipeline solves the problem of limited information being encoded by the encoder. Now based on decoderâ€™s output at each time step, we calculate the weightage given to spatial or temporal part of input. Hence, each vector output by encoder now describes a particular region of the input. Attention mechanism learns to chose which information needs to focussed at a particular time step.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heuritech.files.wordpress.com/2016/01/caption_attention1.png?w=470&quot; alt=&quot;Attention based model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure: Attention based model. This image is copied from blog post by Heuritech [4]&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;notations-for-recurrent-neural-network&quot;&gt;Notations for Recurrent neural network&lt;/h3&gt;

&lt;p&gt;State transition is a function: 
&lt;script type=&quot;math/tex&quot;&gt;h\_{t}^{l-1}, h\_{t-1}^{l} \rightarrow h\_{t}^{l}&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Describing Multimedia Content using Attention-based Encoderâ€“Decoder Networks 
Link : http://arxiv.org/abs/1507.01053&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recurrent Neural Network Regularization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Show, Attend and Tell: Neural Image Caption Generation with Visual Attention&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.heuritech.com/2015/12/01/learning-to-link-images-with-their-descriptions/&quot;&gt;Heuritech blog&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Skip-Thought Vectors&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</content>
   </entry>
   
 
   
   <entry>
     <title>Torch-4 Recurrent Neural Networks, LSTM</title>
     <link href="http://localhost:4000/posts/2016/01/04/torch_4.html"/>
     <updated>2016-01-04T00:00:00-08:00</updated>
     <id>http://localhost:4000/posts/2016/01/04/torch_4</id>
     <content type="html">&lt;h2 id=&quot;why-rnnlstm&quot;&gt;Why RNN/LSTM&lt;/h2&gt;
&lt;p&gt;Traditional neural networks such as convolutional neural networks have been extremely powerful in recognition and classification tasks. On standard datasets they have levelled the human accuracy for object classification. But despite their success, they are not able to analyze sequence of inputs, which contain informaton across time dependencies.&lt;/p&gt;

&lt;p&gt;RNNâ€™s are able to overcome problem with traditional neural networks by making use of sequential information. RNNâ€™s can be thought of as networks with loops, allowing information to flow across time.&lt;/p&gt;

&lt;h2 id=&quot;references-to-understand-mathematical-background-of-recurrent-nets-and-lstm&quot;&gt;References to understand mathematical background of Recurrent Nets and LSTM&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;https://apaszke.github.io/lstm-explained.html&lt;/li&gt;
  &lt;li&gt;https://github.com/karpathy/char-rnn&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=56TYLaQN4N8&quot;&gt;Nando de Freitas Oxford lecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This &lt;a href=&quot;https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/practicals/practical5.pdf&quot;&gt;lecture&lt;/a&gt; by Prof. Nando de Freitas clearly explains the need for nngraph.&lt;/p&gt;

&lt;p&gt;Equations describing LSTM network:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;i\_{t} = sigm(\theta\_{xi} X\_{t} + \theta\_{hi} h\_{t-1} + b\_{i})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\_{t} = sigm(\theta\_{xf} X\_{t} + \theta\_{hf} h\_{t-1} + b\_{f})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o\_{t} = sigm(\theta\_{xo} X\_{t} + \theta\_{ho} h\_{t-1} + b\_{o})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_{t} = sigm(\theta\_{xg} X\_{t} + \theta\_{hg} h\_{t-1} + b\_{g})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c_{t} = f\_{t} \cdot c\_{t-1} + i\_{t} \cdot g\_{t}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{t} = o\_{t} \cdot tanh(c\_{t-1})&lt;/script&gt;

&lt;p&gt;Below is the code snippet describing above equations using Torch.
Please note this code is referenced from this &lt;a href=&quot;https://apaszke.github.io/lstm-explained.html&quot;&gt;excellent blog.&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-lua&quot;&gt;

require 'nn'
require 'nngraph'


local inputs = {}
-- nn.Identity used as a placeholder for input
table.insert(inputs,nn.Identity()())  -- input
table.insert(inputs,nn.Identity()())  -- c at t-1
table.insert(inputs,nn.Identity()())  -- h at t-1

local input = inputs[1]
local prev_c = inputs[2]
local prev_h = inputs[3]

local i2h =  nn.Linear(input_size, 4*rnn_size)(input) 
-- Input to hidden layer. 4 parts would be used for 4 different gates
-- Linear transformation to the incoming data, input. y=Ax+b.

local h2h = nn.Linear(rnn_size,4*rnn_size)(prev_h)
-- hidden to hidden layer. 4 parts would be used for 4 different gates

local preactivations = nn.CAddTable()({i2h,h2h}) 
-- i2h + h2h
-- Portions of this preactivations are for different gates

-- narrow(dim,index,size)
-- narrow  returns a Tensor with the dimension dim is narrowed from index to index+size-1
local pre_sigmoid_chunk = nn.Narrow(2,1,3*rnn_size)(preactivations)
-- we just chose 3 parts of preactivations on which we will apply sigmoid
local in_chunk = nn.Narrow(2,3*rnn_size+1,rnn_size)(preactivations)
-- we just chose 1 part of preactivation on which we will apply tanh (input preactivations)

local all_gates = nn.Sigmoid()(pre_sigmoid_chunk)
local in_transform = nn.Tanh()(in_chunk)

-- seperating 3 gates on which we applied sigmoid
local in_gate = nn.Narrow(2,1,rnn_size)(all_gates)
local forget_gate = nn.Narrow(2,rnn_size+1,rnn_size)(all_gates)
local out_gate = nn.Narrow(2,2*rnn_size+1,rnn_size)(all_gates)


-- next_c equation implementation
local c_forget = nn.CMulTable()({forget_gate,prev_c})
local c_input = nn.CMulTable()({in_gate,in_transform})

local next_c = nn.CAddTable()({c_forget,c_input})

-- next_h equation implementation 
local c_transform = nn.Tanh()(next_c)
local next_h = nn.CMulTable()({out_gate,c_transform})


outputs = {}
table.insert(outputs,next_c)
table.insert(outputs,next_h)

return nn.gModule(inputs,outputs)


&lt;/code&gt;&lt;/pre&gt;
</content>
   </entry>
   
 
   
   <entry>
     <title>Torch-3 Multi-layer perceptron</title>
     <link href="http://localhost:4000/posts/2015/12/26/torch_3.html"/>
     <updated>2015-12-26T00:00:00-08:00</updated>
     <id>http://localhost:4000/posts/2015/12/26/torch_3</id>
     <content type="html">&lt;p&gt;Torch provides collection of blocks or abstract layers for building deep neural networks. 
The nn library abstracts the calculation of errors and backpropogation of the error signals.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-lua&quot;&gt;-- BASIC MODEL
input_size = 3

hidden_size = 150
mlp = nn.Sequential()
mlp:add(nn.Linear(input_size,hidden_size))
mlp:add(nn.Sigmoid())
mlp:add(nn.Linear(hidden_size,1))

criterion = nn.MSECriterion()

trainer = nn.StochasticGradient(mlp,criterion)
trainer.maxIteration = 100

-- INITIALIZE DATA
params = mlp:getParameters()
params:uniform(-0.1,0.1)

trainer:train(dataset)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;preprocessing-the-data&quot;&gt;Preprocessing the data&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-lua&quot;&gt;
require 'torch'
require 'image'
require 'nn'


if not opt then
   print '==&amp;gt; processing options'
   cmd = torch.CmdLine()
   cmd:text()
   cmd:text('SVHN Dataset Preprocessing')
   cmd:text()
   cmd:text('Options:')
   cmd:option('-size', 'small', 'how many samples do we load: small | full | extra')
   cmd:option('-visualize', true, 'visualize input data and weights during training')
   cmd:text()
   opt = cmd:parse(arg or {})
end

-- This would download the dataset if its not already present
print '==&amp;gt; downloading dataset'
www = 'http://torch7.s3-website-us-east-1.amazonaws.com/data/housenumbers/'

train_file = 'train_32x32.t7'
test_file = 'test_32x32.t7'
extra_file = 'extra_32x32.t7'

if not paths.filep(train_file) then
   os.execute('wget ' .. www .. train_file)
end
if not paths.filep(test_file) then
   os.execute('wget ' .. www .. test_file)
end
if opt.size == 'extra' and not paths.filep(extra_file) then
   os.execute('wget ' .. www .. extra_file)   
end



-- training/test size by default small
if opt.size == 'extra' then
   print '==&amp;gt; using extra training data'
   trsize = 73257 + 531131
   tesize = 26032
elseif opt.size == 'full' then
   print '==&amp;gt; using regular, full training data'
   trsize = 73257
   tesize = 26032
elseif opt.size == 'small' then
   print '==&amp;gt; using reduced training data, for fast experiments'
   trsize = 10000
   tesize = 2000
end

-- load train and test data
loaded = torch.load(train_file,'ascii')
trainData = {
   data = loaded.X:transpose(3,4),
   labels = loaded.y[1],
   size = function() return trsize end -- this would be called trainData:size()
}

loaded = torch.load(test_file,'ascii')
testData = {
   data = loaded.X:transpose(3,4),
   labels = loaded.y[1],
   size = function() return tesize end
}

-- convert to float
trainData.data = trainData.data:float()
testData.data = testData.data:float()

-- preprocessing

-- Convert all images to YUV
print '==&amp;gt; preprocessing data: colorspace RGB -&amp;gt; YUV'
for i = 1,trainData:size() do
   trainData.data[i] = image.rgb2yuv(trainData.data[i])
end
for i = 1,testData:size() do
   testData.data[i] = image.rgb2yuv(testData.data[i])
end

-- Name channels for convenience
channels = {'y','u','v'}
print '==&amp;gt; preprocessing data: normalize each feature (channel) globally'
mean = {}
std = {}
for i,channel in ipairs(channels) do
   -- normalize each channel globally by subtracting data from mean and dividing by standard deviation
   mean[i] = trainData.data[{ {},i,{},{} }]:mean()
   std[i] = trainData.data[{ {},i,{},{} }]:std()
   trainData.data[{ {},i,{},{} }]:add(-mean[i])
   trainData.data[{ {},i,{},{} }]:div(std[i])
end

-- Normalize test data, using the training means/stds
for i,channel in ipairs(channels) do
   -- normalize each channel globally:
   testData.data[{ {},i,{},{} }]:add(-mean[i])
   testData.data[{ {},i,{},{} }]:div(std[i])
end


neighborhood = image.gaussian1D(13) -- declaring a gaussian with 13 elements.

-- Local normalization uniformizes the local mean and variance of an image
normalization = nn.SpatialContrastiveNormalization(1, neighborhood, 1):float()

-- Normalize all channels locally:
for c in ipairs(channels) do
   for i = 1,trainData:size() do
      trainData.data[{ i,{c},{},{} }] = normalization:forward(trainData.data[{ i,{c},{},{} }])
   end
   for i = 1,testData:size() do
      testData.data[{ i,{c},{},{} }] = normalization:forward(testData.data[{ i,{c},{},{} }])
   end
end
&lt;/code&gt;&lt;/pre&gt;
</content>
   </entry>
   
 
   
   <entry>
     <title>Torch-2 Data manipulation</title>
     <link href="http://localhost:4000/posts/2015/12/25/torch_2.html"/>
     <updated>2015-12-25T00:00:00-08:00</updated>
     <id>http://localhost:4000/posts/2015/12/25/torch_2</id>
     <content type="html">&lt;h2 id=&quot;basic-data-manipulation-in-torch&quot;&gt;Basic data manipulation in Torch&lt;/h2&gt;
&lt;p&gt;This section descibes basic manipulation with tensors in Torch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-lua&quot;&gt;

t1 = torch.range(1,75)
t1 = t1:resize(3,5,5)

t2 = torch.range(1,25)
t2 = t2:resize(5,5)
print(t1)
print(t2)

--slice using [] operator
print(t1[1]) -- indices start from 1!

t1[1]:fill(2) -- fills the dimesion 1 elements with '2' in t1. 
print(t1)

-- using [{}] operator!

t2_slice1 = t2[{ {},2 }] -- returns  the elements of 2nd column
print(t2_slice1)


t2_slice2 = t2[{ 2,{} }] -- returns the elements of 2nd row
print(t2_slice2)

t2_slice3 = t2[{ {2},{}}] -- converts into a row matrix 1x5
print(t2_slice3)

t2_slice4 = t2[{ {1,5},{4,5}}] -- takes all elements from 1st to 5th row and 4th to 5th column
print(t2_slice4)

t2_slice5 = t2[{{2},{4}}] --  returns a tensor of element 2,4
print(t2_slice5)

t2_slice6 = t2[{2,4}]
print(t2_slice6)

t2_slice7 = t2[{{},{2,-2}}] -- starting from 2nd column till end-2 column
print(t2_slice7)

t2_slice8 = t2[{ -1,-1 }] -- prints element of end,end
print(t2_slice8)


t3 = torch.Tensor(5) -- copying subtensors
print(t3)
t3[{}] = t2[{{},1}]
print(t3)


&lt;/code&gt;&lt;/pre&gt;

</content>
   </entry>
   
 
   
   <entry>
     <title>Torch-1 Fundamentals</title>
     <link href="http://localhost:4000/posts/2015/12/24/torch_1.html"/>
     <updated>2015-12-24T00:00:00-08:00</updated>
     <id>http://localhost:4000/posts/2015/12/24/torch_1</id>
     <content type="html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this blog I aim to analyze the working of Torch and build a simple Neural Network. Much of this work has been adapted from numerous blog posts by inspiring researchers.&lt;/p&gt;

&lt;p&gt;For basic understanding of Lua please refer to &lt;a href=&quot;http://tylerneylon.com/a/learn-lua/&quot;&gt;this blog.&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For building and training networks we use &lt;a href=&quot;https://github.com/torch/nn&quot;&gt;torch/nn&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;â€˜Moduleâ€™ is a class which defines methods for training a neural net. They have two state variables â€˜outputâ€™ and â€˜gradInputâ€™.&lt;/p&gt;

&lt;p&gt;â€˜Outputâ€™ contains the ouput of the module which is computed with last call of inputs to that module. â€˜gradInputâ€™ contains the gradients with respect to the inputs of the module.&lt;/p&gt;

&lt;p&gt;â€˜Containersâ€™ are used to build Neural networks in modular manner using â€˜containerâ€™ classes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-lua&quot;&gt;require 'nn'

-- Linear means we are doing linear transformation y = Ax + b
module = nn.Linear(5,2) -- 5 inputs and 2 output 

-- Use a container sequential which creates a feed-forward fully-connected net
net = nn.Sequential()
net:add(module)

-- Since x is 5x1, the weights A would be 2x5 and bias b would be of size 2x1
print(module.weight)
print(module.bias)

-- Giving random input x and feed forwarding the network to print the output y.
x = torch.rand(5)
print(x)
y = net:forward(x)
print(y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â€˜Criterionsâ€™ are used to compute a gradient based on the loss function used and the target outputs. These are of Classification, Regression, Embedding types depending on the scope of problem we are working on.
These have two important methods called 1) forward 2) backward used to compute the loss and update the weights respectively.&lt;br /&gt;
This &lt;a href=&quot;https://github.com/torch/nn/blob/master/doc/criterion.md#nn.Criterions&quot;&gt;link&lt;/a&gt; descibes use of Criterions with example codes.
Example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-lua&quot;&gt;require 'nn'
-- This performs the backpropogation algo. 
function gradientUpgrade(model, x, y, criterion, learningRate)
    local prediction = model:forward(x) -- calculates the predicted o/p when input is x.
    local err = criterion:forward(prediction, y) -- calculates the error based on criterion 
    local gradOutputs = criterion:backward(prediction, y) -- calculates the gradients 
    model:zeroGradParameters()
    model:backward(x, gradOutputs)
    model:updateParameters(learningRate)
end

model = nn.Sequential()
model:add(nn.Linear(5,1))

x1 = torch.rand(5)
y1 = torch.Tensor({1})

--criterion = nn.MarginCriterion(1)
criterion = nn.AbsCriterion()


for i = 1, 1000 do
    gradientUpgrade(model, x1, y1, criterion, 0.01)
    print('loss after training for x1 = ' .. criterion:forward(model:forward(x1), y1))

end

-- with y1[1] we extract the first value in the tensor
print('prediction for x1 = ' .. model:forward(x1)[1] .. ' expected value ' .. y1[1])

print('loss after training for x1 = ' .. criterion:forward(model:forward(x1), y1))
&lt;/code&gt;&lt;/pre&gt;

</content>
   </entry>
   
 
 
</feed>
