---
layout: post
title: Optimizing LSTM's on GPU with scheduling - Optim-LSTM
category: posts
---

##Team
1. Tejus S, AndrewID : tsiddaga
2. Parth Chadha, AndrewID: pchadha

## Summary
In the Optim-LSTM project, we aim to produce a high-performance implementation of Long-Short Term Memory Network using Domain-Specific Languages such as Halide and/or using custom DSL. This would provide portability across different platforms and architectures.

## Background
LSTM's are a variant of RNN and were designed to tackle the vanishing gradients problem with RNN. 
RNN's when back-propogated over a large number of time-steps, face a problem of diminished value of gradients. With the presence of memory cell in LSTM's, we can have continuous gradient flow which helps in learning long-term dependencies.
Equations describing LSTM network:

$$ i\_{t} = sigm(\theta\_{xi} X\_{t} + \theta\_{hi} h\_{t-1} + b\_{i}) $$

$$ f\_{t} = sigm(\theta\_{xf} X\_{t} + \theta\_{hf} h\_{t-1} + b\_{f}) $$

$$ o\_{t} = sigm(\theta\_{xo} X\_{t} + \theta\_{ho} h\_{t-1} + b\_{o})$$

$$ g_{t} = sigm(\theta\_{xg} X\_{t} + \theta\_{hg} h\_{t-1} + b\_{g}) $$

$$ c_{t} = f\_{t} \cdot c\_{t-1} + i\_{t} \cdot g\_{t} $$

$$ h_{t} = o\_{t} \cdot tanh(c\_{t-1}) $$

The cost involved in evaluating these networks is dominated by Matrix Matrix Multiplication operation, GEMM operation in BLAS libraries. A naive implementation would perform eight matrix-matrix multiplications, 4 with inputs and 4 with previous state vectors. We plan to explore various optimizations around the naive implementation such as Combining Operations, Fusing Point Wise operations etc.

We plan to initially get familiar with Halide and understand more in depth about Domain Specific Languages and understand the limitations of Halide in tasks not meant for Image Processing such as LSTM's. Based on our learning and as mentioned in suggested projects, we plan to implement a custom DSL to incorporate variants of RNN's in the framework. Using the DSL, we aim to use basic blocks from cuDNN or cuBLAS for effecient operations on matrices. 

## Mid-term checkpoint
We explored the implementation of LSTM using Halide and found that the scheduling policies used in Halide is not optimal/suitable for RNN type neural networks. Instead of starting with DSL implementation, we started working on CUDA implementation of LSTM networks as we think this optimization is key for the goal of term project. Once we have an optimized LSTM kernels, we can link them up with a general purpose framework. We have a generic framework setup in C++ which we can use to call our optimized kernels.

###Current implementation:
We used the equations as described in Background section to implement the LSTM structure on CUDA. The current baseline version is a naive implementation using cuBLAS library for General purpose matrix multiplication `(GEMM)`. Although GEMM is highly optimized for GPUs, it does not utilize all the parallelism that can be extracted as the matrix sizes are inherently small.
The current implmentation uses 8 GEMM operations, 4 for multiplying input $x\_{t}$ with $\theta\_{xi},\theta\_{xf},\theta\_{xo},\theta\_{xg}$ and 4 for multiplying $h\_{t-1}$ with $\theta\_{hi},\theta\_{hf},\theta\_{ho},\theta\_{hg}$.

![LSTM layer-wise structure](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/04/image06.png)
*Figure 1: LSTM layer-wise structure*

In our current scheduling policy, we first cover the layer *l*, across the sequence length and then move on to the layer *l+1*. This implementation does not utilize the inherent parallelism provided by the LSTM structure, but it serves as a valid working baseline.

Below are the key optimizations we are trying to implement.

1. Currently our baseline code uses cuBLAS library which does not utilize the complete resource of the GPU and also does not exploit inherent parallelism in LSTM networks. To avoid this problem, we plan to combine multiple matrix multiplications `(Instead of 4 different matrix multiplications)`. We aim to combine 4 matrix multiplication into one GEMM kernel invocation. This should help in exploiting the available GPU resources. This is due to the usage of all the SM's on GPU.
2. As shown in Figure 1, once the computation of first time sequence of first layer `(L0,0)` is completed, in our naive implementation we move on to second time step `(L0,1)` in the same layer. However, since we have the computed output from `(L0,0)`, we can also parallely work on `(L1,0)` cell. This inherent parallelism increases as we progress through the network and more LSTM cells can be computed in parallel. This scheduling policy is expected to give the most boost in GFLOPS.
3. Utilizing the FMA units available on the GPU to reduce the number of times a matrix is accessed from memory. For example, we need to perform two matrix multiplcation and one addition to compute $i\_{t}, f\_{t}, o\_{t}, g\_{t}$. Instead of storing the intermediate results in a buffer, we plan to compute this in the form of $Y = AX + B$ such that the FMA units are utilized. 

### Current Runtime
The runtime for our naive implementation of a 4 layer LSTM with below configurations is 182.5ms. 

LSTM Config:

1. Number of Layers - 4
2. Sequence Length - 100
3. Hidden Unit Size - 512
4. Batch Size - 64

With our proposed optimizations, we expect the runtime to reduce significantly. 

## The Challenge - *Revised*

1. Since this is our introduction to Domain Specifc Languages, we assume the learning curve would be steep and hence we plan to initially spend some time to understand the code structure in Halide.
2. Training of LSTM's using Backpropogation Through Time is a tricky process and might take time to get the gradient calculations right. We can initially try to optimize just for inference using pre-trained network and if the time permits, optimize for backpropogation.

## Goals And Deliverables - *Revised*
###PLAN TO ACHIEVE
1. Implementing a custom DSL for LSTM's 
2. Evaluating the network using library interface from cuBLAS
3. The baseline will consist of running a seperate kernel for each matrix-matrix operation and not performing any optimization such as combination of operations, fusion of operations etc. We plan to freeze the network architecture and hyperparameters after getting a baseline working, and having comparison of optimized version with this network/hyperparameters. We will compare performance using TFLOPS achieved.
4. Optimize only for inference and using a pre-trained network

###HOPE TO ACHIEVE
1. Optimization of back-propogation through time and training deeper networks.
2. Performing hyper-parameter search and optimization to extract maximum parallelism from the underlying hardware
3. Since we are concentrating on the scheduling and optimizing LSTMs for inference, integrating this with Halide/Custom-DSL might be challenging. However, we are providing a generic C++ interface for LSTMs.

## Platform Choice
The GTX 1080 GPU's on GHC.

## Schedule - *Revised*
1. Week 1 - April 10th - 16th
    
    1. Go through Halide source code and learn more about DSL's and understand the limitations of Halide for the use of LSTM's. **DONE**
    2. Have a in-depth understanding of LSTM's implementation **DONE** 

2. Week 2 - April 17th - 23rd
    1. Have a naive working code for LSTM inference **DONE**
    2. Explore DSL's implementation and start implementing a mini-DSL using Python **Pushed to a later stage**

3. Week 3 - April 24th - 30th

    1. Complete DSL implementation **Custom C++ interface**
    2. Explore cuBLAS and cuDNN library and possibly implement a naive basic block code generation
    
4. Week 4 - May 1st - 7th

    1. Test and benchmark the naive implementation
    2. Understand the existing bottlenecks in naive implementation
    3. Perform optimizations on the current implementation for speedup!
    
5. Week 5 - May 8th - 12th
    1. Perform more optimizations and achieve more speedup!
    2. Documentation
    3. If time permits, optimizing for back-propogation and train a network.

##References
1. Optimizing Performance of Recurrent Neural Networks on GPUs
    Link : https://arxiv.org/abs/1604.01946
2. https://devblogs.nvidia.com/parallelforall/  optimizing-recurrent-neural-networks-cudnn-5/
3. http://svail.github.io/diff_graphs/
4. http://svail.github.io/rnn_perf/


 